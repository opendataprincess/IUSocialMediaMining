#Python code Anaconda
import tweepy
type(tweepy)
import win_unicode_console
win_unicode_console.enable()
API_KEY = "rPA7yZAP5902lpj1K4Jd2342N"
API_SECRET = "HhKcMrY0TxLTtkNsie2bmMvtKZTl7wYAMVe56E1EI5FsQMTVeo"
auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)
api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)

#Search a Particular hashtag related to the Venezuela food shortage. Lat/Long default to 28 decimal places.
search_results = api.search(q="#AnaquelesVaciosEnVenezuela", geocode="10.4806, 66.9036,10km")

#importing >25 tweets from a user id
#Note that "montanertwiter" is the top influencer on Twitter in Venezuela (SocialBakers stat)
c = tweepy.Cursor(API.user_timeline, id="montanertwiter")

tweet_texts = []

#Display top 500 tweets --how should this number be determined?
for status in c.items(500):
    tweet_texts.append(status.text)

tweet_texts

#NLP in English Lowercase
tweet_texts =[x.lower() for x in tweet_texts]
tweet_texts[2]

#Split strings into list of individual words
tweet_texts = [x.split() for x in tweet_texts]
tweet_texts[:5]

#Determine word frequency amongst these texts to make a 2D histogram
from collections import Counter
word_frequency = Counter()
for x in tweet_texts: 
    word_frequency.update(x)
word_frequency

word_frequency.most_common(20)
top20 = word_frequency.most_comm(20)
top20
top_words = [x[0] for x in top20]
top_frequencies = [x[1] for x intop20]
#verify separation of data
print(top_words[:5])
print("*"*50)
print(top_frequencies[:5])
x_positions = np.arange(20)
x_positions

plt.bar(x_positions, top_frequencies)
plt.xticks(x_positions, top_words, rotation = 90)
#correct offsetting of words in barchart--move the word to the right
plt.bar(x_positions, top_frequencies)
plt.xsticks(x_positions+0.5, top_words, rotation=90)
plt.title("Word Frequency in Most Recent Trump Tweets")
plt.ylabel("Frequency")
plt.show()

#NLP in Spanish: http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize
import nltk.data
spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')
spanish_tokenizer.tokenize('Hola amigo. Estoy bien.'))

OR 
#Another NLP in Spanish (NLTK CESS Treebank corpus)
from nltk.corpus import cess_esp as cess
from nltk import UnigramTagger as ut
from nltk import BigramTagger as bt

# Read the corpus into a list, 
# each entry in the list is one sentence.
cess_sents = cess.tagged_sents()

# Train the unigram tagger
uni_tag = ut(cess_sents)

sentence = "Hola , esta foo bar ."

# Tagger reads a list of tokens.
uni_tag.tag(sentence.split(" "))

# Split corpus into training and testing set.
train = int(len(cess_sents)*90/100) # 90%

# Train a bigram tagger with only training data.
bi_tag = bt(cess_sents[:train])

# Evaluates on testing data remaining 10%
bi_tag.evaluate(cess_sents[train+1:])

# Using the tagger.
bi_tag.tag(sentence.split(" "))

# Natural Language Toolkit in English: Wordfreq Application
# http://www.nltk.org/book/ch02.html
# Copyright (C) 2001-2016 NLTK Project
# Author: Sumukh Ghodke <sghodke@csse.unimelb.edu.au>
# URL: <http://nltk.org/>

# For license information, see LICENSE.TXT

from matplotlib import pylab
from nltk.text import Text
from nltk.corpus import gutenberg

def plot_word_freq_dist(text):
    fd = text.vocab()

    samples = [item for item, _ in fd.most_common(50)]
    values = [fd[sample] for sample in samples]
    values = [sum(values[:i+1]) * 100.0/fd.N() for i in range(len(values))]
    pylab.title(text.name)
    pylab.xlabel("Samples")
    pylab.ylabel("Cumulative Percentage")
    pylab.plot(values)
    pylab.xticks(range(len(samples)), [str(s) for s in samples], rotation=90)
    pylab.show()

def app():
    t1 = Text(gutenberg.words('melville-moby_dick.txt'))
    plot_word_freq_dist(t1)

if __name__ == '__main__':
    app()

__all__ = ['app']
