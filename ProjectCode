#From CMD window, "pip install pyquery"
#from CMD window, "pip install nltk"
#from CMD window, "pip install stop-words"

import got3
import pandas as pd
#Entire Caracas City
#Set Caracas-wide search criteria for "escasez"
#Transform Tweets to Data Frame using Pandas
tweetCriteria = got3.manager.TweetCriteria()
tweetCriteria.setQuerySearch("escasez geocode:10.4806,-66.9036,10km")
tweetCriteria.setSince("2014-12-01")
tweetCriteria.setUntil("2016-09-30")
escasez_c_tweets = got3.manager.TweetManager.getTweets(TweetCriteria)

#Initialize an empty list to collect tweets
extracted_data = []
for tweet in escasez_c_tweets:
    tid = tweet.id
    #date_created = tweet.created_at
    date_created = tweet.date
    #favorite_count = tweet.favorite_count
    favorite_count = tweet.favorites
    #retweet_count = tweet.retweet_count
    retweet_count = tweet.retweets
    text = tweet.text
    #escasez_c_tweets = [tid, hour_created, text, favorite_count, retweet_count]
    # escasez_c_tweets is an existing object - it's the list of tweet objects
    # from GOT. We don't want to assign the data to escasez_c_tweets
    this_tweet_extracted_information = [tid, date_created, text, favorite_count, retweet_count]
    # tweetCriteria.append(escasez_c_tweets)
    # tweetCriteria is the tweet criteria object, not a list we can append to
    # I made an empty list above (line 2) the for loop to contain our tweet extracted info
    extracted_data.append(this_tweet_extracted_information)
    
# Argument to DataFrame is extracted_data, the list 
# of lists containing extracted data
df = pd.DataFrame(extracted_data)
# df.columns = ["tid", "date_created", "favorite_count", "retweet_count", "text"]
# The name of the columns should correspond to order that we designated for each tweet
# This occurs in line 16 in the above cell.
df.columns = ["tid", "date_created", "text", "favorite_count", "retweet_count"]
df.index = df["tid"]
del df["tid"]
df.shape
df

#Subset the data to retrieve results where favorites count and retweet counts are greater than 5
subset = df[ df['favorite_count'] > 5]
subset = df[ df['retweet_count'] > 5]
subset.shape
subset

#Convert Data frame to CSV file for Raw and Filtered data set results.

df.to_csv("Escasez-Caracas.csv")
subset.to_csv("Escasez-filtered-Caracas.csv")

#Repeat Code above for each neighborhood (Caracas, Baruta, Chacao, El Hatillo, Libertador and Sucre) and 
for each search term ("alimentos", "#AnaquelesenVenezuela", "escasez", "hambre", "VzlaTieneHambre"

#Combine all filtered datasets into 1 CSV file named "Data-1-Tab110216.csv"
#UserId column was removed and a Neighborhood & Search Term Column was added to the CSV file

import pandas as pd
df = pd.DataFrame()
alldatadf = pd
alldatadf = pd.read_csv("Data-1-Tab110216.csv", index_col=0, encoding="latin1")
alldatadf

#Convert the text from the CSV file into a list for NLP processing
#Note we're importing pandas again in the same cell so we don't have to re-run it in jupyter notebook if we 
#close and restart at another time

import pandas as pd
alldatadf = pd.read_csv("Data-1-Tab110216.csv", index_col=0, encoding="latin1") 
text = alldatadf['text']
new = text.values.tolist()
new

#Tokenize, Stem, Remove Stopwords, Lowercase, Remove URLS, Collect URLs into a list
#Use Snowball Stemmer for Spanish text
# Load spanish stopwords
import nltk
from nltk.corpus import stopwords
stopword_list = stopwords.words("spanish")

from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer("spanish")
alldatadf = pd.read_csv("Data-1-Tab110216.csv", index_col=0, encoding="latin1")
alltext = alldatadf['text']

all_processed_texts = []
all_extracted_urls = []

for text in alltext:
    if pd.isnull(text):
        # If t is null, this is an empty row with neighborhood information.
        # There is still one neighborhood row in this file.
        # Add None to both lists to make sure it still has the right amount of rows.
        all_processed_texts.append(None)
        all_extracted_urls.append(None)
    else:
        # Else, we have a properly formatted row, continue as normal
        # This will contain the stemmed words
        stemmed = []
        # This will contain anything we identify as part of a URL
        urls = []
        tokenized = tt.tokenize(text)
        # tokenized is a list of tokens. We iterate through the tokens to 
        # 1. see if it is a URL part and
        # 2. stem it, if it is not
        for token in tokenized:
            if "/" in token or "http" in token or "-" in token:
                # If it contains /, http, or -, it's probably a URL part
                # Append it to the list we are using to collect urls
                urls.append(token)
            else:
                # If it doesn't, it's a normal word.
                # If it's in the stoplist, IGNORE IT
                if token in stopword_list:
                    pass
                else:
                    stemmed_token = stemmer.stem(token)
                    # Append stemmed token to list we are using to collect tokens
                    stemmed.append(stemmed_token)
        # Add the URLs and stemmed words for THIS TWEET to the 
        # lists we are using to collect all results
        all_processed_texts.append(stemmed)
        all_extracted_urls.append(urls)
