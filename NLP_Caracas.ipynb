{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', 'hay', 'café', ',', 'azúcar', ',', 'h.pan', ',', 'jabón', 'para', 'lavar', '.', 'Manteq', ':', '#', 'AnaquelesVaciosEnVenezuela', 'no', 'hay', 'comida', 'en', 'Venezuela..renuncia', 'Nicolas', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laura.kahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"No hay café, azúcar, h.pan, jabón para lavar. Manteq : #AnaquelesVaciosEnVenezuela no hay comida en Venezuela..renuncia Nicolas.\"\n",
    "tokenized_words = word_tokenize(text)\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "com\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"comida\"))#Code adapted from: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Snowball Stemmer    Porter Stemmer      \n",
      "No                  no                  No                  \n",
      "hay                 hay                 hay                 \n",
      "café                caf                 café                \n",
      ",                   ,                   ,                   \n",
      "azúcar              azuc                azúcar              \n",
      ",                   ,                   ,                   \n",
      "h.pan               h.pan               h.pan               \n",
      ",                   ,                   ,                   \n",
      "jabón               jabon               jabón               \n",
      "para                par                 para                \n",
      "lavar               lav                 lavar               \n",
      ".                   .                   .                   \n",
      "Manteq              manteq              manteq              \n",
      ":                   :                   :                   \n",
      "#                   #                   #                   \n",
      "AnaquelesVaciosEnVenezuelaanaquelesvaciosenvenezuelanaquelesvaciosenvenezuela\n",
      "no                  no                  no                  \n",
      "hay                 hay                 hay                 \n",
      "comida              com                 comida              \n",
      "en                  en                  en                  \n",
      "Venezuela..renuncia venezuela..renunci  venezuela..renuncia \n",
      "Nicolas             nicol               nicola              \n",
      ".                   .                   .                   \n"
     ]
    }
   ],
   "source": [
    "#Stemming words from Spanish-language Tweet\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer(\"spanish\")\n",
    "\n",
    "#A list of words to be stemmed\n",
    "word_list = ['No', 'hay', 'café', ',', 'azúcar', ',', 'h.pan', ',', 'jabón', 'para', 'lavar', '.', 'Manteq', ':', '#', 'AnaquelesVaciosEnVenezuela', 'no', 'hay', 'comida', 'en', 'Venezuela..renuncia', 'Nicolas', '.']\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Snowball Stemmer\",\"Porter Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,snowball.stem(word),porter.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "        \"\"\"\n",
    "        Tokenizes sequences of text and stems the tokens.\n",
    "        :param text: String to tokenize\n",
    "        :return: List with stemmed tokens\n",
    "        \"\"\"\n",
    "        tokens = nl.WhitespaceTokenizer().tokenize(text)\n",
    "        tokens = list(set(re.sub(\"[^a-zA-Z\\']\", \"\", token) for token in tokens))\n",
    "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "        tokens = list(set(re.sub(\"[^a-zA-Z]\", \"\", token) for token in tokens))\n",
    "        stems = []\n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "        for token in tokens:\n",
    "            token = stemmer.stem(token)\n",
    "            if token != \"\":\n",
    "                stems.append(token)\n",
    "        return stems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no', 'hay', 'café', ',', 'azúcar', ',', 'h.pan', ',', 'jabón']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lower case the words \n",
    "# Initialize new list\n",
    "words = []\n",
    "# Loop through list tokens and make lower case\n",
    "for word in word_list:\n",
    "    words.append(word.lower())\n",
    "\n",
    "# Print several items from list as sanity check\n",
    "words[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laura.kahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View Spanish Stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# Get English stopwords and print some of them\n",
    "sw = nltk.corpus.stopwords.words('spanish')\n",
    "sw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['café', ',', 'azúcar', ',', 'h.pan', ',', 'jabón', 'lavar', '.', 'manteq']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Spanish Stopwords from Tweet text\n",
    "# Initialize a new list\n",
    "words_ns = []\n",
    "\n",
    "# Add to words_ns all words that are in Tweet text words but are not stopwords\n",
    "for word in words:\n",
    "    if word not in sw:\n",
    "        words_ns.append(word)\n",
    "\n",
    "# Print several list items as sanity check\n",
    "words_ns[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code adapted from -  https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "café\n",
      "\n",
      "azúcar\n",
      "\n",
      "h.pan\n",
      "\n",
      "jabón\n",
      "lavar\n",
      ".\n",
      "manteq\n",
      ":\n",
      "#\n",
      "anaquelesvaciosenvenezuela\n",
      "comida\n",
      "venezuela..renuncia\n",
      "nicolas\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Remove punctuation\n",
    "#Remove comma\n",
    "for item in words_ns:\n",
    "    print(item.strip(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caféazúcarh.panjabónlavarmanteqanaquelesvaciosenvenezuelacomidavenezuela..renuncianicolas'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "sentence = ''.join([letter for letter in words_ns if letter not in punctuation])\n",
    "sentence\n",
    "#This removes all punctuation but joins all the words together rather than keeping them separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laura.kahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['No',\n",
       " 'hay',\n",
       " 'café',\n",
       " 'azúcar',\n",
       " 'h',\n",
       " 'pan',\n",
       " 'jabón',\n",
       " 'para',\n",
       " 'lavar',\n",
       " 'Manteq',\n",
       " 'AnaquelesVaciosEnVenezuela',\n",
       " 'no',\n",
       " 'hay',\n",
       " 'comida',\n",
       " 'en',\n",
       " 'Venezuela',\n",
       " 'renuncia',\n",
       " 'Nicolas']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize and Remove Punctuation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_sent = tokenizer.tokenize(text)\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no', 'hay', 'café', 'azúcar', 'h', 'pan', 'jabón', 'para', 'lavar']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lower case the words \n",
    "# Initialize new list\n",
    "words = []\n",
    "# Loop through list tokens and make lower case\n",
    "for word in tokenized_sent:\n",
    "    words.append(word.lower())\n",
    "\n",
    "# Print several items from list as sanity check\n",
    "words[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['café',\n",
       " 'azúcar',\n",
       " 'h',\n",
       " 'pan',\n",
       " 'jabón',\n",
       " 'lavar',\n",
       " 'manteq',\n",
       " 'anaquelesvaciosenvenezuela',\n",
       " 'comida',\n",
       " 'venezuela',\n",
       " 'renuncia',\n",
       " 'nicolas']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "processed_text = [word for word in words if word not in stopwords.words('spanish')]\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'café', 'azúcar', 'h', 'pan', 'jabón', 'lavar', 'manteq', 'anaquelesvaciosenvenezuela', 'comida', 'venezuela', 'renuncia', 'nicolas'\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert processed_text list to string\n",
    "new = str(processed_text).strip('[]')\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'café', 'azúcar', 'hay', 'pan', 'jabón', 'lavar', 'manteq', 'anaquelesvacios', 'comida', 'venezuela', 'renuncia', 'nicolas'\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace term 'h' in string with 'hay' (Eng translation - there is)\n",
    "#Code adapted from -  https://www.geeksforgeeks.org/python-string-replace/\n",
    "hay = new.replace(\"h\", \"hay\")\n",
    "#Replace term 'anaquelesvaciosenvenzuela' with 'anaqueles' - note \n",
    "final = hay.replace(\"anaquelesvaciosenvenezuela\",\"anaquelesvacios\")\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Snowball Stemmer    Porter Stemmer      \n",
      "café                caf                 café                \n",
      "azúcar              azuc                azúcar              \n",
      "hay                 hay                 hay                 \n",
      "pan                 pan                 pan                 \n",
      "jabón               jabon               jabón               \n",
      "lavar               lav                 lavar               \n",
      "manteq              manteq              manteq              \n",
      "anaqueles           anaquel             anaquel             \n",
      "vacios              vaci                vacio               \n",
      "comida              com                 comida              \n",
      "venezuela           venezuel            venezuela           \n",
      "renuncia            renunci             renuncia            \n",
      "nicolas             nicol               nicola              \n"
     ]
    }
   ],
   "source": [
    "#Stemming words from Spanish-language Tweet\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer(\"spanish\")\n",
    "\n",
    "\n",
    "#A list of words to be stemmed\n",
    "word_list = ['café', 'azúcar', 'hay', 'pan', 'jabón', 'lavar', 'manteq', 'anaqueles', 'vacios','comida', 'venezuela', 'renuncia', 'nicolas']\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Snowball Stemmer\",\"Porter Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,snowball.stem(word),porter.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
