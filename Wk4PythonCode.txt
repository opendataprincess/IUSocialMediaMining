#IU639- Social Media Mining
#Week 4 M4- Getting Data from Twitter
#python code

#Navigate to C directory on machine to install tweepy
#In Windows search box, type "cmd"
#Install tweepy on machine
pip install tweepy

#Python code Anaconda
import tweepy
type(tweepy)
API_KEY = "rPA7yZAP5902lpj1K4Jd2342N"
API_SECRET = "HhKcMrY0TxLTtkNsie2bmMvtKZTl7wYAMVe56E1EI5FsQMTVeo"
auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)
api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
user = api.get_user("IUBloomington")
user
#Get User Attributes
user.name
user.location
user.time_zone
user.friends_count
user.followers_count
#How to know Attributes for an Object
dir(user)
user.created_at
user.name
#This method provides a list of all followers for a particular user and saved as iufollowers
#iufollowers is an iterative
iufollowers = user.followers()
#Print all iufollowers
iufollowers
for f in iufollowers[:10]:
    print(f.name)
    print(f.description)
    print("*"*50)
#Finding the locations for each account IU follows
geolocations = []
for f in user.friends():
    l = f.location
    geolocations.append(l)
geolocations[:25]:
#Each tweet has a URL number at the end of the URL, it's ID
#Put the number in to get the latest status from that account
status = api.get_status(773643148878807040)
dir(status)
status.text
status.source
status.retweet_count
status.favorite_count
u = status.author
u.name
u.description
status.author
#An abbreviated method to getting the description of who wrote a particular status
status.author.description
#Methods don't take any arguments in parenthesis
#Save in variable called 'retweets'
retweets = status.retweets()
for r in retweets[:5]:
    print(r.text)

#List of individuals who retweeted this tweet
for r in retweets[:5]:
    print("Who:")
    print(r.author.screen_name)

#Searching in Tweepy
#q is the search term
search_results = api.search(q="#IUBB")
for status in search_results[:5]:
    print(status.text)
    print(status.created_at)
    print("*"*50)
#Results from "status.created_at" are listed in reverse chronological order in UTC time zone

search_results = api.search(q="#rio")
for status in search_results[:5]:
    print(status.text)
    print("*"*50)

#Limit search results to English language
search_results = api.search(q="#rio", lang="en")
for status in search_results[:5]:
    print(status.text)
    print("*"*50)

#Limit search results to Paris, France Location
search_results = api.search(q="#rio", geocode="48.86, 2.35, 20km")
for status in search_results[:5]:
    print(status.text)
    print("*"*50)

#What's trending in the US right now
trending_us = api.trends_place(23424977)
#This is a list with one object
trending_us
#Remove the object
theobject = trending_us[0]
theobject

thetrends = theobject['trends']
thetrends

first_trend = thetrends[0]
first_trend['name']
#The resulting output is the hashtag trending right now in the U.S.

first_trend['tweet_volume']
first_trend['url']
#Is the content promoted as an ad?
first_trend['promoted_content']
#If you receive no output to this command, it is NOT promoted content

for trend in thetrends[:]:
    print(trend['name'], trend['tweet_volume'])
#Results will be anything trending in the last 24 hours
#any results with "none" are new trends and there are no statistics yet

#Combine all these lines of code using WOEID Lookup (woeid.rosselliot.co.nz) for Russia
trends_russia = api.trends_place(23424936)
#get the first item, then get trends and then get first five
for trend in trends_russia[0]['trends'][:5]:
    print(trends['name'])

#Iterating through Large Lists
search_results = api.search(q="#StarTrek50")
len(search_results)

user = api.get_user("IUBloomington")
followers = user.followers()
#How many followers are in this list?
len(followers)
#Why is the result 20?

user.followers_count
#Twitter doesn't provide the full list at the same time
#It only shows you some of the results
#If you want to see more results, you must request additional results
#Rate limit is the number of requests you can make every 15 minutes
#Tweepy has API parameters that helps us with rate limits

c= tweepy.Cursor(api.search, q="Rio", lang="en")
tweet_store = []
for status in c.items(500):
    statustext = status.text
    tweet_store.append(statustext)
len(tweet_store)
tweet_store[:100]
#Making a Cursor allows you to get the large number of datasets you want using
#Pagination
#See also: http://docs.tweepy.org/en/v3.5.0/cursor_tutorial.html

#M4 Saving your data
import tweepy
API_KEY = ""
API_SECRET = ""
auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)
api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
clinton_tweets = []
trump_tweets = []
for status in tweepy.Cursor(api.user_timeline, id="HillaryClinton").items(100):
    clinton_tweets.append((status.text, status.favorite_count, status.retweet_count, status.source))
    clinton_tweets[0]

#Saving the raw data onto the computer
import pickle as pkl
#Saving the file with variable & open function on your computer
pkl.dump(clinton_tweets, open("clinton.pkl", "wb"))
pkl.dump(trump_tweets, open("trump.pkl", "wb"))

#Importing file from computer
del clinton_tweets
del trump_tweets
new_clinton = pkl.load(open("clinton.pkl", "rb"))


comments_with_scores = []
for comment in user.get_comments(limit=50):
    comments_with_scores.append((comment.body, comment.score))

#In order to capture Twitter feeds with special characters (i.e.-unicode), do the following
#In Windows, type "cmd" to bring up system window
#Type "pip install win-unicode-console"
#Here's the workaround code in Python Anaconda
import win_unicode_console
win_unicode_console.enable()























